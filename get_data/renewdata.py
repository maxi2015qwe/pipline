import requests
import random
from bs4 import BeautifulSoup
import psycopg
from datetime import datetime, timezone
import time
import random


keyword = "BTC"
page = 0
max_pages = 111
all_data = []
stop = False


def crawl_utoday_page(keyword="BTC", page=0):

    url = f"https://u.today/search/node?keys={keyword}&_wrapper_format=html&page={page}"
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}

    res = requests.get(url, headers=headers)
    if res.status_code != 200:
        print(f"âš ï¸ ç¬¬ {page} é¡µè¯·æ±‚å¤±è´¥")
        return [] 

    soup = BeautifulSoup(res.text, "html.parser")

    search_result= soup.find("div", class_="search-result")
    if not search_result:
        print(f"é¡µæ²¡æœ‰æ•°æ®")
        return []
    
    items = search_result.find_all("div", class_="news__item")
    if not items:
        print(f'æ²¡æœ‰æ•°æ®"')
        return []

    page_data = []

    for item in items:
        # é“¾æ¥
        body = item.find("a", class_="news__item-body")
        link = body.get("href") if body else None

        # æ ‡é¢˜
        title_tag = item.find("div", class_="news__item-title")
        title = title_tag.get_text(strip=True) if title_tag else None

        # æ—¥æœŸ
        datetag = item.find("div", class_="humble")
        if datetag:
            raw_date = datetag.get_text(strip=True)
            try:
                dt = datetime.strptime(raw_date, "%b %d, %Y - %H:%M")
                dt = dt.replace(tzinfo=ZoneInfo("Europe/Berlin"))
                dt = dt.astimezone(timezone.utc)

            except Exception:
                dt = raw_date
        else:
            dt = None

        # ä½œè€…
        author_tag = item.find("a", class_="humble humble--author")
        author = author_tag.get_text(strip=True) if author_tag else None

        page_data.append({
            "title": title,
            "link": link,
            "author": author,
            "date": dt
        })

    print(f"âœ… ç¬¬ {page} é¡µè·å– {len(page_data)} æ¡æ–°é—»ã€‚")
    return page_data     


def get_last_update_date():
    """
    ä» PostgreSQL æ•°æ®åº“ä¸­è¯»å–æœ€è¿‘ä¸€æ¡æ–°é—»çš„æ—¥æœŸ
    """
    dbconn = os.getenv("DBCONN")  # Lambda ç¯å¢ƒå˜é‡ä¸­ä¿å­˜è¿æ¥å­—ç¬¦ä¸²
    conn = psycopg.connect(dbconn)
    cur = conn.cursor()
    cur.execute("SELECT MAX(date) FROM news;")   # âš ï¸ æ ¹æ®ä½ çš„è¡¨åå­—æ®µæ”¹
    last_date = cur.fetchone()[0]
    print(f"ğŸ“Œ æ•°æ®åº“ä¸­æœ€æ–°çš„æ–°é—»æ—¥æœŸ: {last_date}")
    cur.close()
    conn.close()

    # æ²¡æ•°æ®æ—¶é»˜è®¤æŠ“å–æœ€è¿‘ 3 å¤©
    if not last_date:
        last_date = datetime.now(timezone.utc).date() - timedelta(days=3)
    else:
        last_date = last_date.date()  # è½¬æˆçº¯æ—¥æœŸ

    return last_date






def lambda_handler(event, context):
    keyword = event.get("keyword", "BTC")
    max_pages = int(event.get("max_pages", 111))
    last_date = get_last_update_date()
    print(f"ğŸ“Œ ä» {last_date} ä¹‹åçš„æ–°é—»å¼€å§‹çˆ¬å–ã€‚")

    # today= datetime.now(timezone.utc).date()
    page = 0
    all_data = []
    stop = False


    while not stop and page < max_pages:

        page_data = crawl_utoday_page(keyword, page)

        # å¦‚æœæ²¡æ•°æ®å°±åœæ­¢
        if not page_data:
            print("âœ… æ²¡æœ‰æ›´å¤šæ•°æ®ï¼Œçˆ¬è™«ç»“æŸã€‚")
            break


            # æŒ‰æ—¥æœŸè¿‡æ»¤ï¼Œåªè¦ä»Šå¤©çš„
        for item in page_data:
            if isinstance(item["date"], str):
                try:
                    item["date"] = datetime.fromisoformat(item["date"].replace("Z", "+00:00"))
                except Exception:
                    continue  # æ ¼å¼é”™è¯¯è·³è¿‡
            news_date = item["date"].astimezone(timezone.utc).date()

            if news_date > last_date:
                all_data.append(item)
            elif news_date <= last_date:
                stop = True  # ä¸€æ—¦å‡ºç°æ˜¨å¤©çš„æ–°é—»å°±åœ
                print(f"â¹ é‡åˆ° {news_date} (â‰¤ {last_date})ï¼Œåœæ­¢çˆ¬å–ã€‚")
                break

        # åˆå¹¶æ•°æ®
        # all_data.extend(page_data)

        # ç¿»é¡µ
        page += 1

        # éšæœºå»¶è¿Ÿï¼Œé˜²æ­¢è¢«å°
        time.sleep(random.uniform(1, 3))

    print(f"å…±çˆ¬å– {len(all_data)} æ¡æ–°é—»ã€‚")
    return all_data

    # for i, item in enumerate(all_data, start=1):
    #     print(f"\nğŸ“° ç¬¬ {i} æ¡")
    #     print(f"æ ‡é¢˜: {item['title']}")
    #     print(f"ä½œè€…: {item['author']}")
    #     print(f"æ—¥æœŸ: {item['date']}")
    #     print(f"é“¾æ¥: {item['link']}")
